#!/usr/bin/env python3
"""
Evaluate all guardrails against go-explore attacker (DEFENSE EVALUATION ONLY).

This script evaluates how well guardrails defend against attacks by running
the baseline go-explore attacker against each guardrail and measuring:
- Breaches (successful attacks that got through)
- False positive rate (benign operations blocked)
- Overall defense score

NOTE: This is DEFENSE evaluation only. Attack scoring is separate and involves
running YOUR attack algorithm against the baseline (allow-all) guardrail.

Output: guardrail_evaluation_results.md
"""

import sys
from pathlib import Path
import importlib.util
import io
import contextlib
from datetime import datetime

# Add repository root to path
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))

from aicomp_sdk.core.env import SandboxEnv
from aicomp_sdk.attacks.baselines.attacker_goexplore import AttackAlgorithm as GoExploreAttacker
from aicomp_sdk.scoring import score_attack, score_defense
from aicomp_sdk.core.predicates import eval_predicates
from evaluation import eval_defense

# Output markdown file
output_file = repo_root / "guardrail_evaluation_results.md"

fixtures = repo_root / "fixtures"

# Ensure secret.txt exists
secret_file = fixtures / "file_seed" / "secret.txt"
if not secret_file.exists():
    secret_file.write_text("SECRET_API_KEY=sk_live_test\nPASSWORD=test123\nTOKEN=admin123")

# CRITICAL: Unset API keys to force use of vulnerable deterministic agent
import os
old_openai_key = os.environ.pop("OPENAI_API_KEY", None)
old_gpt_oss_path = os.environ.pop("GPT_OSS_MODEL_PATH", None)

# Define guardrails to test (excluding simple due to module issues)
GUARDRAILS = [
    ("perfect", "examples/guardrails/guardrail_perfect.py"),
    ("optimal", "examples/guardrails/guardrail_optimal.py"),
    ("prompt_injection", "examples/guardrails/guardrail_prompt_injection.py"),
    ("taint_tracking", "examples/guardrails/guardrail_taint_tracking.py"),
    ("reference", "examples/guardrails/guardrail.py"),
]

# Dataflow tested separately
DATAFLOW_GUARDRAIL = ("dataflow", "examples/guardrails/guardrail_dataflow.py")

# Markdown output buffer
md_output = []

def md_print(text=""):
    """Print to both console and markdown buffer."""
    print(text)
    md_output.append(text)


def load_guardrail(filepath):
    """Dynamically load a guardrail from a file."""
    spec = importlib.util.spec_from_file_location("guardrail_module", filepath)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module.Guardrail()


def load_guardrail_class(filepath):
    """Load guardrail CLASS (not instance) from file."""
    spec = importlib.util.spec_from_file_location("guardrail_module", filepath)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module.Guardrail


def test_guardrail(name, guardrail_path, attack_seconds=30):
    """Test a guardrail against go-explore attacker using official eval_defense."""
    md_print(f"\n### Testing: **{name}**")
    md_print()
    md_print("[INFO] Using deterministic vulnerable agent (testing mode)")
    
    # Load guardrail CLASS for eval_defense
    guardrail_class = load_guardrail_class(guardrail_path)
    
    # Use official eval_defense which tests both attacks and false positives
    # Suppress verbose GO-EXPLORE logging
    md_print(f"- **Attack duration:** {attack_seconds}s")
    
    captured_output = io.StringIO()
    with contextlib.redirect_stdout(captured_output):
        defense_report = eval_defense(guardrail_class, attack_seconds)
    
    # Extract metrics
    breaches_found = defense_report["breach_count"]
    false_positives = defense_report["false_positives"]
    benign_trials = defense_report["benign_trials"]
    fp_rate = false_positives / benign_trials if benign_trials > 0 else 0.0
    
    # Calculate defense score
    defense_score = score_defense(defense_report)
    
    # Count attack types from breaches
    attack_types = {}
    for breach in defense_report["breaches"]:
        for pred in breach["predicates"]:
            pred_name = pred.get("predicate", "UNKNOWN")
            attack_types[pred_name] = attack_types.get(pred_name, 0) + 1
    
    # Display results (defense metrics only)
    md_print(f"- **Breaches:** {breaches_found}")
    md_print(f"- **False Positives:** {false_positives}/{benign_trials} ({fp_rate:.1%})")
    md_print(f"- **Defense Score:** {defense_score:.1f}/1000")
    
    if attack_types:
        md_print(f"- **Attack types that breached:**")
        for atype, count in sorted(attack_types.items(), key=lambda x: x[1], reverse=True)[:5]:
            md_print(f"  - `{atype}`: {count} occurrences")
    md_print()
    
    return {
        "name": name,
        "breaches_found": breaches_found,
        "false_positives": false_positives,
        "fp_rate": fp_rate,
        "benign_trials": benign_trials,
        "attack_types": attack_types,
        "defense_score": defense_score,
    }


def main():
    md_print("# Guardrail Defense Evaluation Results")
    md_print()
    md_print(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md_print()
    md_print("**IMPORTANT:** This is DEFENSE evaluation only. It measures how well guardrails block attacks.")
    md_print("Attack scoring (measuring your attack algorithm's effectiveness) is evaluated separately.")
    md_print()
    md_print("## Test Configuration")
    md_print()
    md_print("- **Evaluation Type:** Defense (baseline attack vs. user guardrails)")
    md_print("- **Attacker:** Go-Explore baseline")
    md_print("- **Agent:** Deterministic (testing mode, no API keys)")
    md_print("- **Attack budget:** 30 seconds per guardrail (60s for dataflow)")
    md_print("- **Go-Explore config:** max_turns=8, branch_batch=12")
    md_print()
    
    results = []
    
    # Test all guardrails (except dataflow)
    md_print("## Phase 1: Standard Guardrails")
    md_print()
    
    for name, filepath in GUARDRAILS:
        try:
            guardrail_path = repo_root / filepath
            result = test_guardrail(name, str(guardrail_path), attack_seconds=30)
            results.append(result)
        except Exception as e:
            md_print(f"\n‚ùå **Error testing {name}:** {e}")
            md_print()
    
    # Test dataflow separately with more time
    md_print("---")
    md_print()
    md_print("## Phase 2: Dataflow Guardrail (Extended Testing)")
    md_print()
    md_print("The dataflow guardrail receives extended attack time (60s) for comprehensive evaluation.")
    md_print()
    
    try:
        dataflow_name, dataflow_filepath = DATAFLOW_GUARDRAIL
        dataflow_path = repo_root / dataflow_filepath
        dataflow_result = test_guardrail(dataflow_name, str(dataflow_path), attack_seconds=60)
        results.append(dataflow_result)
    except Exception as e:
        md_print(f"\n‚ùå **Error testing dataflow:** {e}")
        md_print()
    
    # Summary
    md_print("---")
    md_print()
    md_print("## Summary")
    md_print()
    md_print("| Guardrail | Defense Score | Breaches | FP Rate | Benign Trials |")
    md_print("|-----------|---------------|----------|---------|---------------|")
    
    # Sort by defense score (descending)
    results.sort(key=lambda x: x["defense_score"], reverse=True)
    
    for result in results:
        name = result["name"]
        defense_score = result["defense_score"]
        breaches = result["breaches_found"]
        fp_rate_display = f"{result['fp_rate']:.1%}"
        benign_trials = result["benign_trials"]
        md_print(f"| {name:<15} | {defense_score:>7.1f}/1000 | {breaches:<8} | {fp_rate_display:<7} | {benign_trials:<13} |")
    
    md_print()
    
    # Best and worst performers
    md_print("## Analysis")
    md_print()
    
    if results:
        best = results[0]
        worst = results[-1]
        
        md_print(f"### üèÜ Best Defense: **{best['name']}**")
        md_print()
        md_print(f"- **Defense Score:** {best['defense_score']:.1f}/1000")
        md_print(f"- **Breaches:** {best['breaches_found']}")
        md_print(f"- **False Positive Rate:** {best['fp_rate']:.1%}")
        if best['attack_types']:
            top_attacks = ', '.join([f"`{k}`" for k in list(best['attack_types'].keys())[:3]])
            md_print(f"- **Most common breach types:** {top_attacks}")
        md_print()
        
        md_print(f"### ‚ö†Ô∏è Weakest Defense: **{worst['name']}**")
        md_print()
        md_print(f"- **Defense Score:** {worst['defense_score']:.1f}/1000")
        md_print(f"- **Breaches:** {worst['breaches_found']}")
        md_print(f"- **False Positive Rate:** {worst['fp_rate']:.1%}")
        if worst['attack_types']:
            top_attacks = ', '.join([f"`{k}`" for k in list(worst['attack_types'].keys())[:3]])
            md_print(f"- **Most common breach types:** {top_attacks}")
        md_print()
    
    # Attack type analysis
    md_print("### Attack Type Analysis")
    md_print()
    
    attack_type_summary = {}
    for result in results:
        for atype, count in result['attack_types'].items():
            if atype not in attack_type_summary:
                attack_type_summary[atype] = []
            attack_type_summary[atype].append(result['name'])
    
    md_print("| Attack Type | Bypassed Guardrails | Success Rate |")
    md_print("|-------------|---------------------|--------------|")
    
    for atype, guardrails in sorted(attack_type_summary.items(), key=lambda x: len(x[1]), reverse=True):
        success_rate = f"{len(guardrails)}/{len(results)}"
        bypassed_list = ', '.join([f"`{g}`" for g in guardrails[:3]])
        if len(guardrails) > 3:
            bypassed_list += ", ..."
        md_print(f"| `{atype}` | {bypassed_list} | {success_rate} |")
    
    md_print()
    md_print("---")
    md_print()
    md_print("**End of Report**")
    
    # Write to file
    with open(output_file, 'w') as f:
        f.write('\n'.join(md_output))
    
    print(f"\n\n‚úÖ Results written to: {output_file}")


if __name__ == "__main__":
    main()
